{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_batcher import get_batch_generator\n",
    "from vocab import get_glove, get_char_embed\n",
    "from modules import RNNEncoder, masked_softmax\n",
    "from tensorflow.python.ops.rnn_cell import DropoutWrapper\n",
    "\n",
    "from bilm import Batcher, BidirectionalLanguageModel, weight_layers\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define statistics and hyperparameters\n",
    "batch_size = 2\n",
    "hidden_size = 7\n",
    "context_len = 100\n",
    "question_len = 25\n",
    "embedding_size = 50\n",
    "char_size = 20\n",
    "num_of_char = 72\n",
    "max_word_len = 20\n",
    "dropout = 0.2\n",
    "\n",
    "#Define path\n",
    "train_context_path =  \"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squadV2/data/train.context\"\n",
    "train_qn_path = \"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squadV2/data/train.question\"\n",
    "train_ans_path = \"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squadV2/data/train.span\"\n",
    "dev_qn_path = \"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squadV2/data/dev.question\"\n",
    "dev_context_path = \"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squadV2/data/dev.context\"\n",
    "dev_ans_path = \"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squadV2/data/dev.span\"\n",
    "\n",
    "elmo_dir = \"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squad/data/elmo\"\n",
    "\n",
    "class FLAGS(object):\n",
    "    def __init__(self, batch_size, hidden_size, context_len, question_len, embedding_size, char_size, num_of_char, max_word_len, dropout, elmo_dir):\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.context_len = context_len\n",
    "        self.question_len = question_len\n",
    "        self.embedding_size = embedding_size\n",
    "        self.char_size = char_size\n",
    "        self.num_of_char = num_of_char\n",
    "        self.max_word_len = max_word_len\n",
    "        self.dropout = dropout\n",
    "        self.elmo_dir = elmo_dir\n",
    "\n",
    "FLAGS = FLAGS(batch_size, hidden_size, context_len, question_len, embedding_size, char_size, num_of_char, max_word_len, dropout, elmo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GLoVE vectors from file: /Users/lam/Desktop/Lam-cs224n/Projects/qa/squadV2/data/glove.6B.50d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:10<00:00, 37280.82it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_path = \"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squadV2/data/glove.6B.50d.txt\"\n",
    "emb_matrix, word2id, id2word = get_glove(glove_path, FLAGS.embedding_size)\n",
    "char2id, id2char = get_char_embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAModel(object):\n",
    "    def __init__(self, FLAGS, id2word, word2id, emb_matrix, id2char, char2id):\n",
    "        self.FLAGS = FLAGS\n",
    "        self.id2word = id2word\n",
    "        self.word2id = word2id\n",
    "        self.emb_matrix =  emb_matrix\n",
    "        self.id2char = id2char\n",
    "        self.char2id = char2id\n",
    "        \n",
    "        self.batcher = Batcher(\"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squad/data/elmo/elmo_vocab.txt\", 50)\n",
    "        self.filters = [(5,10)] #change back to 100 after\n",
    "        \n",
    "        self.options_file = \"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squad/data/elmo/elmo.json\"\n",
    "        self.weight_file = \"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squad/data/elmo/lm_weight.hdf5\"\n",
    "        \n",
    "        with tf.variable_scope(\"QAModel\"):\n",
    "            self.add_placeholders()\n",
    "            self.add_embedding_layer(emb_matrix)\n",
    "        \n",
    "    def add_placeholders(self):\n",
    "        self.context_ids = tf.placeholder(tf.int32)\n",
    "        self.context_mask = tf.placeholder(tf.int32)\n",
    "        self.qn_ids = tf.placeholder(tf.int32)\n",
    "        self.qn_mask = tf.placeholder(tf.int32)\n",
    "        self.ans_span = tf.placeholder(tf.int32, shape=[None, 2])\n",
    "        \n",
    "        #NOTE:CHANGE\n",
    "        #self.context_char = tf.placeholder(tf.int32, shape=[None, self.FLAGS.context_len, self.FLAGS.max_word_len])\n",
    "        #self.qn_char = tf.placeholder(tf.int32, shape=[None, self.FLAGS.question_len, self.FLAGS.max_word_len])\n",
    "        #The following two may not be necessary\n",
    "        #self.context_char_mask = tf.placeholder(tf.int32, shape=[None, self.FLAGS.context_len, self.FLAGS.max_word_len])\n",
    "        #self.qn_char_mask = tf.placeholder(tf.int32, shape=[None, self.FLAGS.question_len, self.FLAGS.max_word_len])\n",
    "        self.context_elmo = tf.placeholder('int32', shape=[None, None, 50])\n",
    "        self.qn_elmo = tf.placeholder('int32', shape=[None, None, 50])\n",
    "        \n",
    "        # Add a placeholder to feed in the keep probability (for dropout).\n",
    "        # This is necessary so that we can instruct the model to use dropout when training, but not when testing\n",
    "        self.keep_prob = tf.placeholder_with_default(1.0, shape=())\n",
    "    \n",
    "    def add_embedding_layer(self, emb_matrix):\n",
    "        with tf.variable_scope(\"embeddings\"):\n",
    "            #set to constant so its untrainable\n",
    "            embedding_matrix = tf.constant(emb_matrix, dtype=tf.float32, name=\"emb_matrix\") # shape (400002, embedding_size)\n",
    "\n",
    "            # Get the word embeddings for the context and question,\n",
    "            self.context_embs = tf.nn.embedding_lookup(embedding_matrix, self.context_ids)\n",
    "            self.qn_embs = tf.nn.embedding_lookup(embedding_matrix, self.qn_ids)\n",
    "\n",
    "        #self.add_char_embedding_layer()\n",
    "\n",
    "    def add_elmo_embedding_layer(self, options_file, weight_file, output_use=False):\n",
    "        \"\"\"\n",
    "        Adds ELMo lstm embeddings to the graph.\n",
    "\n",
    "        Inputs:\n",
    "            options_file: json_file for the pretrained model\n",
    "            weight_file: weights hdf5 file for the pretrained model\n",
    "            output_use: determine if use elmo in output of biRNN (default False)\n",
    "        \"\"\"\n",
    "        #Build biLM graph\n",
    "        bilm = BidirectionalLanguageModel(options_file, weight_file)\n",
    "        context_embeddings_op = bilm(self.context_elmo)\n",
    "        question_embeddings_op = bilm(self.qn_elmo)\n",
    "\n",
    "        # Get an op to compute ELMo (weighted average of the internal biLM layers)\n",
    "        # Our SQuAD model includes ELMo at both the input and output layers\n",
    "        # of the task GRU, so we need 4x ELMo representations for the question\n",
    "        # and context at each of the input and output.\n",
    "        # We use the same ELMo weights for both the question and context\n",
    "        # at each of the input and output.\n",
    "        #compute the final ELMo representations.\n",
    "        self.elmo_context_input = weight_layers('input', context_embeddings_op, l2_coef=0.001)['weighted_op'] #(batch size, context size, ????)\n",
    "        with tf.variable_scope('', reuse=True):\n",
    "            # the reuse=True scope reuses weights from the context for the question\n",
    "            self.elmo_question_input = weight_layers(\n",
    "                'input', question_embeddings_op, l2_coef=0.001\n",
    "            )['weighted_op']\n",
    "\n",
    "        if output_use:\n",
    "            self.elmo_context_output = weight_layers(\n",
    "                'output', context_embeddings_op, l2_coef=0.001\n",
    "            )['weighted_op']\n",
    "            with tf.variable_scope('', reuse=True):\n",
    "                # the reuse=True scope reuses weights from the context for the question\n",
    "                self.elmo_question_output = weight_layers(\n",
    "                    'output', question_embeddings_op, l2_coef=0.001\n",
    "                )['weighted_op']\n",
    "    \n",
    "    \n",
    "    def run_train_iter(self, session, batch):\n",
    "        input_feed = {}\n",
    "        input_feed[self.context_ids] = batch.context_ids\n",
    "        input_feed[self.context_mask] = batch.context_mask\n",
    "        \n",
    "        #NOTE: CHANGE added context_char\n",
    "        #input_feed[self.context_char] = batch.context_char\n",
    "        input_feed[self.context_elmo] = self.batcher.batch_sentences(batch.context_tokens)\n",
    "        \n",
    "        input_feed[self.qn_ids] = batch.qn_ids\n",
    "        input_feed[self.qn_mask] = batch.qn_mask\n",
    "        \n",
    "        #NOTE: CHANGE added qn_char\n",
    "        #input_feed[self.qn_char] = batch.qn_char\n",
    "        input_feed[self.qn_elmo] = self.batcher.batch_sentences(batch.qn_tokens)\n",
    "        \n",
    "        input_feed[self.ans_span] = batch.ans_span\n",
    "        input_feed[self.keep_prob] = 1.0 - self.FLAGS.dropout # apply dropout\n",
    "        \n",
    "        \n",
    "        output_feed = [self.elmo_context_input]\n",
    "        sess.run(output_feed, feed_dict=input_feed)\n",
    "        for i in output_feed:\n",
    "            print(i.shape)\n",
    "    \n",
    "    def train(self, session, train_context_path, train_qn_path, train_ans_path, dev_qn_path, dev_context_path, dev_ans_path):\n",
    "        #self.add_elmo_embedding_layer(self.options_file, self.weight_file)\n",
    "        for batch in get_batch_generator(self.word2id, self.char2id, train_context_path, train_qn_path, train_ans_path, self.FLAGS.batch_size, self.FLAGS.context_len, self.FLAGS.question_len, self.FLAGS.max_word_len, discard_long=True):\n",
    "            self.sample_batch = batch\n",
    "            \n",
    "            self.run_train_iter(session, batch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING SKIP CONNECTIONS\n",
      "USING SKIP CONNECTIONS\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable bilm/char_embed already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/anaconda3/envs/cs231n/lib/python3.6/site-packages/bilm-0.1-py3.6.egg/bilm/model.py\", line 266, in custom_getter\n    return getter(name, *args, **kwargs)\n  File \"/anaconda3/envs/cs231n/lib/python3.6/site-packages/bilm-0.1-py3.6.egg/bilm/model.py\", line 336, in _build_word_char_embeddings\n    initializer=tf.random_uniform_initializer(-1.0, 1.0)\n  File \"/anaconda3/envs/cs231n/lib/python3.6/site-packages/bilm-0.1-py3.6.egg/bilm/model.py\", line 281, in _build\n    self._build_word_char_embeddings()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f6c5a9391eea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mqa_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_context_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_qn_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ans_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_qn_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_context_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_ans_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#variables_names =[v.name for v in tf.trainable_variables()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#values = sess.run(variables_names)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-e8c52fc2abdc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, session, train_context_path, train_qn_path, train_ans_path, dev_qn_path, dev_context_path, dev_ans_path)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_context_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_qn_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ans_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_qn_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_context_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_ans_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_elmo_embedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_batch_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_context_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_qn_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ans_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_word_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscard_long\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-e8c52fc2abdc>\u001b[0m in \u001b[0;36madd_elmo_embedding_layer\u001b[0;34m(self, options_file, weight_file, output_use)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m#Build biLM graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mbilm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectionalLanguageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcontext_embeddings_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbilm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_elmo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mquestion_embeddings_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbilm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqn_elmo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/cs231n/lib/python3.6/site-packages/bilm-0.1-py3.6.egg/bilm/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, ids_placeholder)\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0membedding_weight_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_weight_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0muse_character_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_character_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                     max_batch_size=self._max_batch_size)\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/cs231n/lib/python3.6/site-packages/bilm-0.1-py3.6.egg/bilm/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options, weight_file, ids_placeholder, use_character_inputs, embedding_weight_file, max_batch_size)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bilm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/cs231n/lib/python3.6/site-packages/bilm-0.1-py3.6.egg/bilm/model.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_character_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_word_char_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_word_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/cs231n/lib/python3.6/site-packages/bilm-0.1-py3.6.egg/bilm/model.py\u001b[0m in \u001b[0;36m_build_word_char_embeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0;34m\"char_embed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_embed_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                     \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             )\n\u001b[1;32m    338\u001b[0m             \u001b[0;31m# shape (batch_size, unroll_steps, max_chars, embed_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    415\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m\"constraint\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"constraint\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m       return _true_getter(\n",
      "\u001b[0;32m/anaconda3/envs/cs231n/lib/python3.6/site-packages/bilm-0.1-py3.6.egg/bilm/model.py\u001b[0m in \u001b[0;36mcustom_getter\u001b[0;34m(getter, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_weight_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             )\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membedding_weight_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    731\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 733\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    734\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable bilm/char_embed already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/anaconda3/envs/cs231n/lib/python3.6/site-packages/bilm-0.1-py3.6.egg/bilm/model.py\", line 266, in custom_getter\n    return getter(name, *args, **kwargs)\n  File \"/anaconda3/envs/cs231n/lib/python3.6/site-packages/bilm-0.1-py3.6.egg/bilm/model.py\", line 336, in _build_word_char_embeddings\n    initializer=tf.random_uniform_initializer(-1.0, 1.0)\n  File \"/anaconda3/envs/cs231n/lib/python3.6/site-packages/bilm-0.1-py3.6.egg/bilm/model.py\", line 281, in _build\n    self._build_word_char_embeddings()\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "options_file = \"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squad/data/elmo/elmo.json\"\n",
    "weight_file= \"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squad/data/elmo/lm_weight.hdf5\"\n",
    "\n",
    "qa_model = QAModel(FLAGS, id2word, word2id, emb_matrix, id2char, char2id)\n",
    "qa_model.add_elmo_embedding_layer(options_file, weight_file)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    qa_model.train(sess, train_context_path, train_qn_path, train_ans_path, dev_qn_path, dev_context_path, dev_ans_path)\n",
    "    #variables_names =[v.name for v in tf.trainable_variables()]\n",
    "    #values = sess.run(variables_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_file = \"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squad/data/elmo/emlo.json\"\n",
    "weight_file= \"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squad/data/elmo/lm_weight.hdf5\"\n",
    "\n",
    "bilm = BidirectionalLanguageModel(options_file, weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batcher sentence shape:  (2, 13, 50)\n",
      "First sentence len:  9\n",
      "Second sentence len:  11\n",
      "----------------------------------------\n",
      "batcher sentence shape:  (2, 17, 50)\n",
      "First question len:  15\n",
      "Second question len:  8\n"
     ]
    }
   ],
   "source": [
    "batcher = Batcher(\"/Users/lam/Desktop/Lam-cs224n/Projects/qa/squad/data/elmo/elmo_vocab.txt\", 50)\n",
    "\n",
    "raw_context = [\n",
    "    'Pretrained biLMs compute representations useful for NLP tasks .',\n",
    "    'They give state of the art performance for many tasks .'\n",
    "]\n",
    "tokenized_context = [sentence.split() for sentence in raw_context]\n",
    "context_ids = batcher.batch_sentences(tokenized_context)\n",
    "qn_elmo = batcher.batch_sentences(qa_model.sample_batch.qn_tokens)\n",
    "\n",
    "print(\"batcher sentence shape: \", context_ids.shape)\n",
    "print(\"First sentence len: \", len(tokenized_context[0]))\n",
    "print(\"Second sentence len: \", len(tokenized_context[1]))\n",
    "print(\"--\"*20)\n",
    "print(\"batcher sentence shape: \", batcher.batch_sentences(qa_model.sample_batch.qn_tokens).shape)\n",
    "print(\"First question len: \", len(qa_model.sample_batch.qn_tokens[0]))\n",
    "print(\"Second question len: \", len(qa_model.sample_batch.qn_tokens[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qn_elmo[0][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_model.sample_batch.context_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refilling batches...\n",
      "Refilling batches took 1.08 seconds\n"
     ]
    }
   ],
   "source": [
    "for batch in get_batch_generator(word2id, char2id, train_context_path, train_qn_path, train_ans_path, batch_size, context_len, question_len, max_word_len, discard_long=True):\n",
    "    sample = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_ids shape:  (2, 85)\n",
      "context_mask shape:  (2, 85)\n",
      "qn_ids shape:  (2, 11)\n",
      "qn_mask shape:  (2, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"context_ids shape: \", sample.context_ids.shape)\n",
    "print(\"context_mask shape: \", sample.context_mask.shape)\n",
    "\n",
    "print(\"qn_ids shape: \", sample.qn_ids.shape)\n",
    "print(\"qn_mask shape: \", sample.qn_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first context tokens len:  85\n",
      "second context tokens len:  85\n",
      "first question tokens len:  7\n",
      "second question tokens len:  11\n"
     ]
    }
   ],
   "source": [
    "print(\"first context tokens len: \", len(sample.context_tokens[0]))\n",
    "print(\"second context tokens len: \", len(sample.context_tokens[1]))\n",
    "\n",
    "print(\"first question tokens len: \", len(sample.qn_tokens[0]))\n",
    "print(\"second question tokens len: \", len(sample.qn_tokens[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
